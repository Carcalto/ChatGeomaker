import streamlit as st
import os
from groq import Groq  # Certifique-se de que o Groq est√° corretamente importado.
from llama_index.llms.groq import Groq as LlamaGroq
from llama_index.core.llms import ChatMessage
from langchain.chains import LLMChain
from langchain.llms import GroqLLM
from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessage, HumanMessagePromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferWindowMemory

def main():
    # Configura√ß√µes iniciais da p√°gina
    st.set_page_config(page_icon="üí¨", layout="wide", page_title="Chat Avan√ßado com Mem√≥ria e RAG")
    st.markdown(f'<span style="font-size: 78px;">üß†</span>', unsafe_allow_html=True)  # √çcone grande
    st.title("Aplicativo de Chat Avan√ßado para Educa√ß√£o")
    st.write("Bem-vindo ao sistema avan√ßado de chat!")

    # Configura√ß√µes de ambiente e API
    api_key = st.secrets.get("GROQ_API_KEY", "your_api_key_here")
    groq_client = Groq(api_key=api_key)
    llama_groq = LlamaGroq(model="llama3-70b-8192", api_key=api_key)

    # Prepara√ß√£o da mem√≥ria de conversa√ß√£o
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    memory = ConversationBufferWindowMemory(k=5)

    # Configura√ß√µes de modelo e prompt
    system_prompt = st.text_area("Defina o prompt do sistema:", "Digite aqui...")
    model_choice = st.selectbox("Escolha um modelo:", ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768", "gemma-7b-it"])

    # Entrada de pergunta do usu√°rio
    user_question = st.text_input("Insira sua pergunta aqui:")
    if user_question:
        # Adicionando a pergunta √† mem√≥ria
        memory.save_context({'input': user_question}, {'output': ''})

        # Cria√ß√£o do prompt
        prompt_template = ChatPromptTemplate([
            SystemMessage(system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            HumanMessagePromptTemplate(template="{human_input}")
        ])
        conversation_chain = LLMChain(
            llm=GroqLLM(api_key=api_key, model_name=model_choice),
            prompt=prompt_template,
            memory=memory
        )

        # Previs√£o e resposta
        response = conversation_chain.predict(human_input=user_question)
        st.session_state.chat_history.append({'role': 'user', 'content': user_question})
        st.session_state.chat_history.append({'role': 'assistant', 'content': response})
        st.write("Resposta do Chatbot:", response)

    # Exibi√ß√£o do hist√≥rico de mensagens
    for message in st.session_state.chat_history:
        role = "ü§ñ" if message['role'] == 'assistant' else "üë§"
        st.write(f"{role} {message['content']}")

if __name__ == "__main__":
    main()
